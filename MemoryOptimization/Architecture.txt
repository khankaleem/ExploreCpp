CPU Caches:
  
  


Virtual Memory:
      






Cache Bypass:
  Processors provide support for non temporal write operations
  Gcc provides streaming instrinsics for x86-64
  Writes are written to write combine buffer of the processor
  Memory is not loaded into cache, but streamed straight to RAM
  Memory order is relaxed for these writes
  e.g void _mm_stream_si128(int *p, __m128i a);
  
  We can also do non temporal reads, they dont bring memory into cache
  They are implemented using streaming load buffers
  We can load a full 64 bytes at once in the streaming load buffers
  e.g __m128i _mm_stream_load_si128 (__m128i *p);





Optimizing L1d:
  1. Matrix Mulitplication Example:  
    Split the matrix into 64 * 64 blocks for the mulitplication
    This ensures that every cache line loaded is utilized to the full before eviction

  2. Struct Layout: Pack the object in as little Cache lines as Possible, to improve cache efficiency
    1. Move the structure element which is most likely to be the critical word to the front
    2. In a structure Arrange elements in the order in which they are likely to be accessed
    3. For a structure spanning multiple cache lines, each cache line block should be rearranged to follow 1/2.
    
    If the Object itself is not aligned then 1/2 are useless. 
    To align an Object:
      1. For dynamically allocated: use operator new Struct, automatically aligns the object, 
         For statically allocated we can use alignas(64)
      2. We can mark the alignemnt of the Struct with alignas(64), will work for arrays as well

    Within a stack compiler can take care about the alognment,
    but the caller must align the stack ptr
    but VLAs will spoil the party and will need dynamic alignement by the callee
    Hence Alignment of stack variables comes at a cost.
    
    x86-64 has a hard requirement that stack pointer must be 16 byte aligned for SSE instructions to work
    The caller must align the stack ptr to atleast 16bytes

  3. Seperate Irrelevant data from struct to increase cache performance

  4. Minimize Conflict Misses: Higher Assocaitivity decreases miss rate but increases search time
      L1d Cache is small, but not small enough to be Fully Associative, We primarily need to have lightning speed of Ascess, so lower associativy is ideal, usuall 8way
      L2d Cache is larger, so we can increase associativty to decrease conflict misses, usually 16way

      L1d is VIPT
        So programmer has control on what goes in a set.
        If L1d is 8 - way assocaitaive and size is 32KB, 64 Byte Cache Line
        Number of sets = 32KB / 64B / 8 = 64 sets -> addressable using 6 bits
        So after every 12bits, i.e 4096 bytes data maps to existing sets -> we can avoid this
      
    



Optimizing L1i:
  Code is linear between jumps, in this period processor prefetcher instructions effieciently
  Jumps disturb this layout
  1. Jump target might not be statically determined
  2. Even if it is statically determined, fetch might take a long time
  This stalls execution and negatively impacts performance 
  Instructions are generally cached in decoded form

  Hardware Branch Predictor : Determine the target of jump many cycles before
  Maxmime Instruction Cache Performance:
  1. Reduce Code footprint : Balance with with optimizations like loop unrolling , and inlining.
      O3 -> deals with loop optimizations, and function inlining
      Inlining Pros:
        Inlining allows compiler to optimize large chunks of code at a time, whch enables machien code with better ILP
              
      Inlining Cons:  If compiler inline function size is not good for the program then it shoudl be lowered

        Increases size causing cache pressure
        Inlining will cause code not to be in L1i 
        Inlining might cause branch predictor not to work efficiently since it has already seen the code
        
  2. Linear Code execution
      Whenever conditional execution is used, and its losided, static prediction can fail, causing pipline flush.
      To avoid we can use compiler hints
      #define unlikely(expr) __builtin_expect(!!(expr), 0)
      #define likely(expr) __builtin_expect(!!(expr),
      These hints allow compiler to optimize the binary memory layout
      Pros:
        1. Better cold start
        2. Fall through - no rediredction to a far away new address
        3. Cache Line Waste is minimized by exiling cold code to a different page, and increasing spatial locality of cold code
        Block reordering takes place.
        Cold code is stored in .text.unlikely

  3. Align Code
    We need to align to optimize prefetching and decoding
      1. start of functions
      2. start of blocks reached through jumps
      3. maybe loops




Prefetching:
Used to hide Main Memory Latency
  Hardware Prefetching:
    Process Agnostic
    Hardware Prefetcher is per L1 / L2 / L3 Cache
    Uses streams to track cache line access patterns
    It can recognize stride patterns
    Very effective in sequential access patterns
    Does not work accross Page boundaries, and random access

  Software Prefetcher:
    __builtin_prefetch (const void *addr, rw, locality)
    rw = 0 : Acquire in Exclusive / Shared state
    rw = 1 : Acquire in Exclusive / Shared state, then write to it
    locality = 0 (Non temporal) / 1 (>=L1) / 2(>=L2) / 3 (>= L3)


Direct Cache Access:
  NIC -> Use DMA to put incoming packet data into RAM
         DCA flag is set in these transfers, Processor listens via FSB to DCA flag, 
         and adds data to Cache
         Intel replaced this with DDIO, where the destination is not RAM but compulsoraliy in  L3.







Multithreaded Optimizations:

  Concurrency Optimization:
    False Sharing -> 
       Same cache line and different cores write to it,
       they all fight for E access over the same cache line
       There is a constant Cache Line Ping Pong
       Massive Bus Traffic, and Processing delays accept the one having E access.
       Big performance killer on SMP Machines.
       Cache Coherency Messages and the Cache Line Data has to travel across the system bus or interconnect
  
       Solutions:
         1. Seperate read only data -> mark const, or move it to particular section (attribute section e.g ".data.ro")
         2. Put variables which are accessed only by 1 thread in the same struct : Optimize localilty
         3. Variables read / write should be aligned to cache lines size
         4. Use TLS : Can cause high memory foot print, and impacts thread launch time

  Atomicity Optimizations:
        Two threads writing to the same location dont wait for E access. The read the value and when E access is available
        they write the value. -> not thread safe
        Atomic Operations:  Make the above read-write atomic.
  
        CAS -> Costly, multiple RFOs
        Fetch Add -> Only one RFO request : for arithematic this is preferred
    
  BandWidth Optimizations:
      
        Each processor has a maximum bandwidth to the memory shared by cores and hyper threads
        Available memory bandwidth can limit performance

        Massive bandwidth usage: 
          Two threads on different processors using the same dataset. 
          Which means each data has to be read twice from memory
          Put them in the same processor

        Thread Affinity: assign a thread to one or more cores
            int pthread_setaffinity_np(pthread_t th, size_t size, const cpu_set_t *cpuset);

          Two threads working on seperate data: Put them on different processors
          











