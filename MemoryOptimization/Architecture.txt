                 ┌────────────────────────────────┐
                 │           Intel Socket          │
                 │       (1 CPU Package)           │
                 │                                │
                 │   ┌────────────────────────┐   │
                 │   │           Die0         │   │
                 │   │                        │   │
                 │   │   ┌────────────────┐   │   │
                 │   │   │   Shared L3    │◄──┼───┼─ LLC (data only)
                 │   │   │     (LLC)      │   │   │
                 │   │   └────────────────┘   │   │
                 │   │                        │   │
                 │   │   ┌────────────────┐   │   │
                 │   │   │      Core 0    │   │   │
                 │   │   │                │   │   │
                 │   │   │  L1i TLB       │   │   │
                 │   │   │  L1d TLB       │   │   │
                 │   │   │  ────────────  │   │   │
                 │   │   │  L1i Cache     │   │   │
                 │   │   │  L1d Cache     │   │   │
                 │   │   │  ────────────  │   │   │
                 │   │   │  L2 Cache      │   │   │
                 │   │   │  ────────────  │   │   │
                 │   │   │  L2 TLB        │   │   │
                 │   │   └────────────────┘   │   │
                 │   │                        │   │
                 │   │   ┌────────────────┐   │   │
                 │   │   │      Core 1    │   │   │
                 │   │   │                │   │   │
                 │   │   │  L1i TLB       │   │   │
                 │   │   │  L1d TLB       │   │   │
                 │   │   │  ────────────  │   │   │
                 │   │   │  L1i Cache     │   │   │
                 │   │   │  L1d Cache     │   │   │
                 │   │   │  ────────────  │   │   │
                 │   │   │  L2 Cache      │   │   │
                 │   │   │  ────────────  │   │   │
                 │   │   │  L2 TLB        │   │   │
                 │   │   └────────────────┘   │   │
                 │   │                        │   │
                 │   │   ┌────────────────┐   │   │
                 │   │   │      Core N    │   │   │
                 │   │   │                │   │   │
                 │   │   │  L1i TLB       │   │   │
                 │   │   │  L1d TLB       │   │   │
                 │   │   │  ────────────  │   │   │
                 │   │   │  L1i Cache     │   │   │
                 │   │   │  L1d Cache     │   │   │
                 │   │   │  ────────────  │   │   │
                 │   │   │  L2 Cache      │   │   │
                 │   │   │  ────────────  │   │   │
                 │   │   │  L2 TLB        │   │   │
                 │   │   └────────────────┘   │   │
                 │   │                        │   │
                 │   └────────────────────────┘   │
                 │                                │
                 └────────────────────────────────┘


kernel tries to schedule / allocate emory on the same Numa node but does not guarantee
      NUMA Node 0                  NUMA Node 1
   ┌─────────────────┐          ┌─────────────────┐
   │ CPUs: 0–15      │◄────────►│ CPUs: 16–31     │
   │ Local DRAM      │  slower  │ Local DRAM      │
   └─────────────────┘          └─────────────────┘
        ↑       ↑                     ↑       ↑
        |       |                     |       |
     tasks   pages                 tasks   pages


CPU Caches:
  Cache : SRAM, Resources administrated by the processors
  Cache stores copies of data / instructions which is likely to be used soon, it works beause all programs have temporal, and spatial locality
  Most programs although have working set bigger than the Cache
  CPU is not connected to Main Memory, All Loads and Stores must go through the Cache
  CPU and Cache connection is very fast

  L1 Cache and RAM speed difference is very high, so we need more layers of Caches 
  Most CPUs have L1d, L1i, L2(Unified instrunction and data), L3(Shared) across cores) Cache

  A Cache Line is 64 bytes, The memory bus width(e.g 8Bytes) determines the transfers per Cache Line
  
  [T bits for the tag][S bits for Cache set][6 bits for Cache Line Offset]
  Tag - Data(Cache Line) - Valid/Invalid - DirtBit - ....
  L1d use VIPT : virtual indexing plus physical tag

  Exclusive or Inclusive Caches
  L1  -  4 Cycles
  L2  - 14 Cycles
  L3  - 50 Cycles
  RAM - 250+ Cycles
  CPUs hide the Memory Access Latency
  Loads - OOO, Prefetching - (Bandwidth / Limited ILP can hurt)
  Stores - Store Buffers

  Associativity:
    Fully Associative :  Less Speed, Less Conflicts Misses - Best for Small Caches
    K Set Associative :  More Speed, More Conflict Misses - Best for Large Caches - Combines Fully / Direct Benefits
    L1 -> 8 Way L2 / L3 -> 16 Way, Beyond 16 Hardware Complexity rises  
    L1 TLB -> Usually Fully Asssociative, L2 TLB -> 8 Way
    Direct Mapped(K = 1) : Best Speed, Worst Conflict Misses

  Cache Effects:
    Array Sequential Access:
      WS < L1d : L1 access latency around 4 cycles
      WS < L2d : Misses will icnrease but Harware prefetcher will do its job and hide the L2 access latency, so latency ~ 8 cycles
      WS > L2d : Misses will increase more, but Harware prefetcher will again do its job and hide the RMA access latency ~ 10 cycles
                 Hardwware prefetcher has to be really aggressive in this case to completly hide the RAM latency
                
      Element Size is a single Cache Line:
        WS < L1d : same
        WS < L2d : L2 penallty will show up, as a new Cache Line is touched on every access, and prefetcher cant keep up
        WS > L2d : L2 + RAM penalty will show up becasue of reason above
        Another reason prefetcher cant hide RAM latency will be Page faults because hardware prefetcher cant work outside page boundaries
        Also if WSS increases this can cause TLB misses, resulting in a page table walk, which itself resides in RAM.


  Write Behaviour:
      Cache is just an extension of memory, if Cache line is modifed it means memory is modified, and all processors must agree on this 
      Write through Cache: Immediately flushed to memory : This would increase the bus traffic 
      Write Back : Dirty bit tracks the updated cache line, and is finally writeen when appropriate. 

      Cache Coherency:
        MESI Cache Protocol
        Processors Snoop on the bus, operations done by a processor and announced, address of the lines are visible on the bus.

        M: 
          Other processor requests read, State chagnes to shared, and contents are sent to the requesting processor. Memory controller writes to Main Memory as well, otherwise state cannot be shared.
          Other processor requests write: Lead to RFO request, currently processor invalidates its copy
      E: 
        If lcoal processor wants to write no RFO -> this is an optimization

        RFOs are required when thread is migrated to another core, atomic operations, synchronization, false sharing 
        MESI transaciton cannot happen until all processors have have had a chance to reply, 
        Bus contention is possible, NUMA systems will ahve high latency, 


  Hyper Threading:
    Hyper threading is useful only when wait time(Due To Cache Miss or IO) for a thread is large. otherwise it is useless
    Cache hit rate must be low enough, given reduced cache size, and new hit rate dont impact

  





Virtual Memory:
  Makes a proces think it is alone in the system
  MMU does the transaltion, CPU fills in the pages

  [Virtual Page Number][Page offset - 12bits for 4kb pages]
      |
       _ Index into the Page Table to find the physical frame number

  Page table is stored in Main Memory
  Page table can be huge, so multilevel page tables are used
  [][9 bits][9 bits][9 bits][9 bits][9 bits][12 bits for offset]
  2**9 entries = 512, 512 * 8bytes = 4KB -> fits in one page
  Inner most level 1 page can address 512 * 4KB = 2MB of memmory

  Main Memory access is required for each direcotry access, which can be slow,
  So L1d and higher level also cache it, but still access can be extremely slow.
  This requires TLB, which stores the complet virtual page nu,ber to physical page number mapping
  L1 ITLB
  L1 DTLB
  L2 is unified

  On context swtihc of a process TLB is not flushed, isntead ASID are used
  Increasing 4KB page size can lead to fragmentation.
  But will increse TLB hit rate, and also decrease Page table hence page table tree levels.

  Use huge pages (using hugetlbfs fielsystem)- setup at boot time, this just creates a fixed number of huge physical pages.
  


Cache Bypass:
  Processors provide support for non temporal write operations
  Gcc provides streaming instrinsics for x86-64
  Writes are written to write combine buffer of the processor
  Memory is not loaded into cache, but streamed straight to RAM
  Memory order is relaxed for these writes
  e.g void _mm_stream_si128(int *p, __m128i a);
  
  We can also do non temporal reads, they dont bring memory into cache
  They are implemented using streaming load buffers
  We can load a full 64 bytes at once in the streaming load buffers
  e.g __m128i _mm_stream_load_si128 (__m128i *p);



Optimizing L1d:
  1. Matrix Mulitplication Example:  
    Split the matrix into 64 * 64 blocks for the mulitplication
    This ensures that every cache line loaded is utilized to the full before eviction

  2. Struct Layout: Pack the object in as little Cache lines as Possible, to improve cache efficiency
    1. Move the structure element which is most likely to be the critical word to the front
    2. In a structure Arrange elements in the order in which they are likely to be accessed
    3. For a structure spanning multiple cache lines, each cache line block should be rearranged to follow 1/2.
    
    If the Object itself is not aligned then 1/2 are useless. 
    To align an Object:
      1. For dynamically allocated: use operator new Struct, automatically aligns the object, 
         For statically allocated we can use alignas(64)
      2. We can mark the alignemnt of the Struct with alignas(64), will work for arrays as well

    Within a stack compiler can take care about the alognment,
    but the caller must align the stack ptr
    but VLAs will spoil the party and will need dynamic alignement by the callee
    Hence Alignment of stack variables comes at a cost.
    
    x86-64 has a hard requirement that stack pointer must be 16 byte aligned for SSE instructions to work
    The caller must align the stack ptr to atleast 16bytes

  3. Seperate Irrelevant data from struct to increase cache performance

  4. Minimize Conflict Misses: Higher Assocaitivity decreases miss rate but increases search time
      L1d Cache is small, but not small enough to be Fully Associative, We primarily need to have lightning speed of Ascess, so lower associativy is ideal, usuall 8way
      L2d Cache is larger, so we can increase associativty to decrease conflict misses, usually 16way

      L1d is VIPT
        So programmer has control on what goes in a set.
        If L1d is 8 - way assocaitaive and size is 32KB, 64 Byte Cache Line
        Number of sets = 32KB / 64B / 8 = 64 sets -> addressable using 6 bits
        So after every 12bits, i.e 4096 bytes data maps to existing sets -> we can avoid this
      
    



Optimizing L1i:
  Code is linear between jumps, in this period processor prefetcher instructions effieciently
  Jumps disturb this layout
  1. Jump target might not be statically determined
  2. Even if it is statically determined, fetch might take a long time
  This stalls execution and negatively impacts performance 
  Instructions are generally cached in decoded form

  Hardware Branch Predictor : Determine the target of jump many cycles before
  Ther determine
  Maxmime Instruction Cache Performance:
  1. Reduce Code footprint : Balance with with optimizations like loop unrolling, and inlining.
      O3 -> deals with loop optimizations, and function inlining
      Inlining Pros:
        Inlining allows compiler to optimize large chunks of code at a time, whch enables machien code with better ILP
              
      Inlining Cons:  If compiler inline function size is not good for the program then it shoudl be lowered

        Increases size causing cache pressure
        Inlining will cause code not to be in L1i 
        Inlining might cause branch predictor not to work efficiently since it has already seen the code
        
  2. Linear Code execution
      Whenever conditional execution is used, and its losided, static prediction can fail, causing pipline flush.
      To avoid we can use compiler hints
      #define unlikely(expr) __builtin_expect(!!(expr), 0)
      #define likely(expr) __builtin_expect(!!(expr),
      These hints allow compiler to optimize the binary memory layout
      Pros:
        1. Better cold start
        2. Fall through - no rediredction to a far away new address
        3. Cache Line Waste is minimized by exiling cold code to a different page, and increasing spatial locality of cold code
        Block reordering takes place.
        Cold code is stored in .text.unlikely
     So likely unlikely is used to increase the spatial locality of hot code
     and to improve static prediciton.

  3. Align Code : NoOps are inserted at the begining
    We need to align to optimize prefetching and decoding
      1. start of functions
      2. start of blocks reached through jumps
      3. maybe loops : NoOps can cause delay in fall through of loop

  


Prefetching:
Used to hide Main Memory Latency
  Hardware Prefetching:
    Process Agnostic
    Hardware Prefetcher is per L1 / L2 / L3 Cache
    Uses streams to track cache line access patterns
    It can recognize stride patterns
    Very effective in sequential access patterns
    Does not work accross Page boundaries, and random access

  Software Prefetcher:
    __builtin_prefetch (const void *addr, rw, locality)
    rw = 0 : Acquire in Exclusive / Shared state
    rw = 1 : Acquire in Exclusive / Shared state, then write to it
    locality = 0 (Non temporal) / 1 (>=L1) / 2(>=L2) / 3 (>= L3)


Direct Cache Access:
  NIC -> Use DMA to put incoming packet data into RAM
         DCA flag is set in these transfers, Processor listens via FSB to DCA flag, 
         and adds data to Cache
         Intel replaced this with DDIO, where the destination is not RAM but compulsoraliy in  L3.







Multithreaded Optimizations:

  Concurrency Optimization:
    False Sharing -> 
       Same cache line and different cores write to it,
       they all fight for E access over the same cache line
       There is a constant Cache Line Ping Pong
       Massive Bus Traffic, and Processing delays accept the one having E access.
       Big performance killer on SMP Machines.
       Cache Coherency Messages and the Cache Line Data has to travel across the system bus or interconnect
  
       Solutions:
         1. Seperate read only data -> mark const, or move it to particular section (attribute section e.g ".data.ro")
         2. Put variables which are accessed only by 1 thread in the same struct : Optimize localilty
         3. Variables read / write should be aligned to cache lines size
         4. Use TLS : Can cause high memory foot print, and impacts thread launch time

  Atomicity Optimizations:
        Two threads writing to the same location dont wait for E access. The read the value and when E access is available
        they write the value. -> not thread safe
        Atomic Operations:  Make the above read-write atomic.
  
        CAS -> Costly, multiple RFOs
        Fetch Add -> Only one RFO request : for arithematic this is preferred
    
  BandWidth Optimizations:
      
        Each processor has a maximum bandwidth to the memory shared by cores and hyper threads
        Available memory bandwidth can limit performance

        Massive bandwidth usage: 
          Two threads on different processors using the same dataset. 
          Which means each data has to be read twice from memory
          Put them in the same processor

        Thread Affinity: assign a thread to one or more cores
            int pthread_setaffinity_np(pthread_t th, size_t size, const cpu_set_t *cpuset);

          Two threads working on seperate data: Put them on different processors
          











