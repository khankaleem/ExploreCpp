CPU Caches:

  


Virtual Memory:




Cache Bypass:




Optimizing L1d:




Optimizing L1i:




Prefetching:
Used to hide Main Memory Latency
  Hardware Prefetching:
    Process Agnostic
    Hardware Prefetcher is per L1 / L2 / L3 Cache
    Uses streams to track cache line access patterns
    It can recognize stride patterns
    Very effective in sequential access patterns
    Does not work accross Page boundaries, and random access

  Software Prefetcher:
    __builtin_prefetch (const void *addr, rw, locality)
    rw = 0 : Acquire in Exclusive / Shared state
    rw = 1 : Acquire in Exclusive / Shared state, then write to it
    locality = 0 (Non temporal) / 1 (>=L1) / 2(>=L2) / 3 (>= L3)


Direct Cache Access:
  NIC -> Use DMA to put incoming packet data into RAM
         DCA flag is set in these transfers, Processor listens via FSB to DCA flag, 
         and adds data to Cache
         Intel replaced this with DDIO, where the destination is not RAM but compulsoraliy in  L3.



Multithreaded Optimizations:
  Concurrency Optimization:
    False Sharing -> 
                     Same cache line and different cores write to it,
                     they all fight for E access over the same cache line
                     There is a constant Cache Line Ping Pong
                     Massive Bus Traffic, and Processing delays accept the one having E access.
                     Big performance killer on SMP Machines.
                     Cache Coherency Messages and the Cache Line Data has to travel across the system bus or interconnect

                     Solutions:
                       1. Seperate read only data -> mark const, or move it to particular section (attribute section e.g ".data.ro")
                       2. Put variables which are accessed only by 1 thread in the same struct : Optimize localilty
                       3. Variables read / write should be aligned to cache lines size
                       4. Use TLS : Can cause high memory foot print, and impacts thread launch time
      
    Atomicity:
          Two threads writing to the same location dont wait for E access. The read the value and when E access is available
          they write the value. -> not thread safe
          Atomic Operations:  Make the above read-write atomic.

          CAS -> Costly, multiple RFOs
          Fetch Add -> Only one RFO request : for arithematic this is preferred
    














